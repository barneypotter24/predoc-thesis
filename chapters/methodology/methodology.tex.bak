% !TeX root = ../../thesis.tex
\chapter{This is methodology}\label{ch:methodology}

% \section{Genomic sequence data}
%
% For both Hepatitis B and Lassa analyses our genomic sequence came from a variety of sources.
% Both analyses were comprised of a majority of sequences pulled directly from publically available sequence databses.
% Additionally, both analyses include a subset of novel sequences generated by their respective study groups that have not yet been released publically.

\section{Phylogeography of contemporary and ancient HBV}

\subsection{Genomic sequence data}
We analyze a total of 1,590 Hepatitis B virus genomes.
These genomes were divided into three datasets based on their HBV subgenotype, such that we had 587 HBV-A genomes, 769 HBV-D genomes, and 234 HBV-E genomes.
Of these, the vast majority (total 1,466: 540 HBV-A, 765 HBV-D, 170 HBV-E) were provided by collaborators following a query of the GenBank sequence database.
Additionally, for two of the subgenotypes we make the addition of recently published genomes isolated from mummies ranging in age from 450 years old to 4,500 years old \cite{muhlemann, ross}.
These genomes help add a small amount of temporal signal into the HBV datasets, which normally show remarkably little, allowing us to infer approximate timing windows of historical events.

\subsection{Preprocessing}

Prior to Bayesian inference, we estimated unrooted maximum likelihood phylogenetic trees using IQ-TREE.
These maximum likelihood trees were used to perform temporal signal analysis using TempEST, allowing us to remove sequences that showed incongruent temporal signal patterns.
The resulting dataset comprised 583 sequences of HBV-A genotype viruses, 764 HBV-D sequences, and 234 sequences HBV-E genotype viruses.
With the inclusion of ancient genomes, the final HBV-A and HBV-D datasets included 587 and 769 total taxa, respectively.

\subsection{Phylogenetic modeling}

We inferred phylogenetic trees for each of our three datasets using MCMC analyses implemented in BEAST1.10.
For all datasets we used the same model specification.
Branch rates were inferred under a lognormal uncorrelated relaxed molecular clock model, allowing us to accommodate clock rate variation between branches of the tree.
Following unsuccessful modelling of demographic history under a Bayesian Skygrid model, we used a constant demographic model as a tree model.
To accommodate the impact of the overlapping regions in the HBV genome on the substitution process of each codon position, we assumed an HKY substitution model without codon partitioning, accommodating among-site rate heterogeneity through a discretized gamma distribution.
We performed at least three independent replicates of our MCMC analysis to ensure proper convergence, with each replicate running for 500 million iterations and subsampling every 10,000th iteration.
We assumed default priors in BEAST for all parameters these analyses except for lognormal substitution rate ($X \sim Lognormal(-11.3474,1.22)$), which was derived from the HBV substitution rate estimated by the researchers who generated the ancient genomes\cite{muhlemann}.
To speed likelihood complutations we used the BEAGLE 3 high-performance computational library.
MCMC chains were run for 500 million iterations and were sampled every 25,000th state.
For each subtype multiple replicates of each subtype were run using different starting seeds in parallel to ensure proper convergence and statistical mixing, and a single combined posterior distribution was formed from all replicates, removing an appropriate proportion (at least 10\%) of each chain as burn-in.
From this combined posterior distribution, 1,000 trees were sampled and used to perform subsequent Bayesian phylogeographic reconstruction in BEAST using empirical tree distributions.

\subsection{Phylogeographic modeling}

Employing the collected empirical tree distributions, we estimated the spatial diffusion dynamics using a Bayesian phylogeographic approach in which the locations are treated as discrete traits, and allowed for different migration rates to and from each location.
This approach conditioed on the geographic locations recorded at the tips of the trees and models the transition history among those locations as a continuous-time Markov chain (CTMC) process to infer the unobserved locations at the internal nodes of each tree.
For these reconstructions, MCMC chains were run for 10 million iterations and sampled every 10,000th state.
Maximum clade credibility (MCC) trees were summarized using TreeAnnotator v1.10 (cite BEAST 1.10) and the resulting trees were visualized in FigTree v1.4.3.
The links between locations that contribute significantly to explaining the migration history were identified using a Bayesian stochastic search variable selection (BSSVS) procedure, and SpreaD3 was used to calculate the Bayes factor (BF) support for these links.
We also estimated the expected number of location state transitions in the ancestral history conditional on the data observed at the tree tips using ``Markov jump" counts, providing a quantitative measure of asymmetry in viral flow between regions.
Since the data consist of a heterogeneous number of sequences for each location which show strong sampling bias between individual countries that can strongly impact the outcome of a discrete phylogeography study.
To mitigate this issue, we opted to group sequences into continental regions, aiming to contain a number of sequences of the same order of magnitude where possible: Africa, the Americas, East South Asia, Europe, and West Central Asia.
We note that for data set E, no samples from East South Asia nor West Central Asia were available.

\section{Lassa phylogenetics and online inference}

\subsection{Adaptable bipartite build framework}

Lassa proves to be a particularly difficult virus to preprocess for the purposes of phylogenetic inference.
In particular, generating an accurate and useful alignment can be very challenging with Lassa due to its high sequence variability.
The challenges from this variability are compounded, as they need to be accounted for both across Lassa's two genomic segments, but also for the two genes on each of the segments, which need to be aligned to each other to provide meaningful results during phylogenetic analysis.
To handle these challenges, we divide our adaptable processing framework into two steps, one for alignment generation, and one for build parameterization and specification.

\subsubsection{Snakemake for build management}

For both of these steps, we employ the use of Snakemake\cite{snakemake}, a flexible workflow management system designed to be used with the Python programing language, to manage build specifications and easily perform online Bayesian inference.
We use this as an alternative to the traditional methods of workflow management---most commonly simple shell scripts or repeated use of command line tools.
The use of Snakemake in our build pipelines affords us three major advantages over previous methods of build management: increased build reproducibility, easy parameterization, and robust error management.

A pipeline is broken into its componenet steps as a series of ``rules"---each encapsulating a single command line directive---that can be parameterized by a single configuration file.
Each rule is defined in terms of its expected input and output as filenames; each rule also specifies a particular directive that should generate the outputs from the input.
For example, one could specify a rule, \texttt{count\_lines}, whose goal was to apply the \texttt{wc -l} command line utility to an \texttt{input.txt} file and generate an \texttt{output.txt} file.
At runtime Snakemake would check that \texttt{output.txt} did not already exist on the filesystem---an important check that prevents costly recomputation.
Next, Snakemake would check if \texttt{input.txt} existed on the filesystem, yielding an error if the file were not present.
Snakemake would then apply the specified command line utility.
Finally, Snakemake would check that \texttt{output.txt} existed in the filesystem, throwing an error if the expected file were not present.
Importantly, Snakemake allows us to generalize these steps so that one's input is determined as a function of the previous step's output, and to apply dynamic changes to the workflow according to user specified variables.
This combination of rules being ``strung" together, plus filesystem checks to determine presence or absence of input and output files, affords us the opportunity to update builds quickly and without performing unnecessary recomputations.

Because each command is centralized into the Snakemake workflow, exact reproduction of past analyses becomes trivial.
Indeed, reproduction of past analyses with minor alterations (e.g. a slightly updated dataset, different MCMC chain length) also becomes trivial, as all parameters are saved in a single configuration JSON file, rather than memorized or stored in shell history.
This is of particular importance because it allows for easy specification of updated builds without running the risk of losing previously performed analyses due to filesystem overwrites (a concern when using shell scripting or command line copy/pasting).
It also ensures that all build parameters and commands that are not explicitly modified by the user are kept constant, ensuring consistency between builds.

\subsubsection{Alignment generation}

The our Lassa analyses began with genomes gathered from GenBank via API query and from previously unpublished genomes from 2019 seasonal outbreaks in Nigeria provided by collaborators (671 L segment, 795 S segment).
Midway through the analysis period we received incoming data updates from collaborators in Nigeria, increasing our dataset sizes (698, L segment, 826 S segment).
Data from collaborators were given to us as Microsoft Excel spreadsheets containing both sequences and metadata, necessitating a preprocessing pipeline that could collate new genomes with those generated by GenBank query.
For each sequence, its genotype was determined according to BLAST search.
Genotype-gene alignments were then built using MAFFT and concatenated ``vertically" across genotypes.
Finally, alignments were concatenated ``horizontally" between the two genes for each segment, padded with \textit{N} characters to allow for variations in length and to buffer between genes.
This process is illustrated in Figure~\ref{fig:alignment_dag}.

\begin{figure}[ht]
  \centering
  \medskip
  \includegraphics[width=.9\textwidth]{alignment_dag}
  \caption[Lassa alignment pipeline]{A directed acyclic graph (DAG) representing the pipeline by which we generate Lassa virus alignments. We first use BLAST on each segment to determine its genotype. We then build gene-specific alignments for each genotype, and concatenate the results ``vertically"---i.e. all alignment files are combined into one. Then, we update the alignments such that all genotypes are as well aligned with each other as possible. Finally, we combine each gene ``horizonally" within each segment---i.e. for each segment and each strain there is a single sequence in the alignment---such that the final result are two final alignment files, one for each genome.}
  \label{fig:alignment_dag}
\end{figure}

\subsubsection{MCMC build specification}

For our Lassa analyses, we created a Snakemake pipeline similar in form to that used for preprocessing.
This pipeline (Fig.~\ref{fig:workflow_dag}) took the output alignments described above as its primary input.
From each, we generated BEAST XML files.
We use a Bayesian Skygrid demographic models for our analyses---though the pipeline is parameterized to easily accomodate both constant population size and Bayesian Skyride demographic models.
Our Skygrid inference used a Hamiltonian Monte Carlo (HMC) operator to allow efficient transition of population size state space.
We assumed an HKY substitution model with codon partitioning for each gene, accommodating among-site rate heterogeneity through a discretized gamma distribution.
We otherwise assume default BEAST values for operators and priors.
The MCMC chain length, logging frequency, and number of parallel replicates were parameterized dynamically though Snakemake, however for the results we present we ran a single chain for $5\times10^7$ iterations, logging every 15,000th state.

For each segment, after its parallel BEAST replicates were completed, we remove burnin from each log (minimum 10\%), and concatenate logs to form one joint posterior sample set.
We perform an identical process for posterior trees.
We use these combined parameter and tree distributions to generate MCC trees for each segment.

\begin{figure}[ht]
  \centering
  \medskip
  \includegraphics[width=.9\textwidth]{workflow_dag}
  \caption[Lassa phylogenetics pipeline]{A directed acyclic graph (DAG) representing the pipeline by which we perform Lassa virus phylogenetic inference. We perform identical workflows for each genomic segment. After BEAST XMLs are generated and run, we remove burnin and analyze the results of combined parameter and tree distributions.}
  \label{fig:workflow_dag}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Keep the following \cleardoublepage at the end of this file,
% otherwise \includeonly includes empty pages.
\cleardoublepage

% vim: tw=70 nocindent expandtab foldmethod=marker foldmarker={{{}{,}{}}}
